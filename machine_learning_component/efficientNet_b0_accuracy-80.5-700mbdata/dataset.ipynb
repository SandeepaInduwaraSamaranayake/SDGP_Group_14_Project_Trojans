{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "import torchvision.datasets as datasets \n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader              # IMPORT WeightedRandomSampler TO DEAL WITH INBALANCE CLASSES.\n",
    "                                                                            # THIS WILL MAKE OUR DATA INPUT PIPELINE MORE GENERAL, WHICH SUITES FOR ANY CASE.\n",
    "                                                                            # IF WE HAVE MORE EXAMPLE IMAGES FOR A SPECIFIC BREED, THEN IT WOULD BE HANDLED BY THIS DATA PIPELINE.\n",
    "                                                                            # IN OUR CASE DIFFERENT DOG BREEDS HAVE DIFFERENT NUMBER OF EXAMPLE IMAGES. SO THIS IS NEEDED.\n",
    "                                                                            # IF DATASET IS PERFECTLY EVEN, THEN THIS WOULD BE AN UNNECESSARY THING TO DO.\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess images(Resizing, Cropping, Flipping(both vertical and horizontal))\n",
    "### Create a weightRandomSampler to deal with imbalanced classes. \n",
    "### Load train data and validation data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_loaders(train_dir, dev_dir, batch_size, image_size):\n",
    "    print(\"getting loaders\")\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(( 300,300 )),\n",
    "        transforms.RandomCrop(( image_size, image_size )),\n",
    "        transforms.RandomHorizontalFlip( p = 0.5 ),\n",
    "        transforms.RandomVerticalFlip( p = 0.05 ),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    dev_transforms = transforms.Compose([\n",
    "        transforms.Resize(( image_size, image_size )),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root = train_dir, transform = train_transforms)                  # CREATING THE TRAIN DATASET.\n",
    "       \n",
    "    dev_dataset = datasets.ImageFolder(root = dev_dir,  transform = train_transforms)                     # CREATE THE DEV DATASET.\n",
    "\n",
    "    val_loader = DataLoader( dev_dataset, batch_size = batch_size, num_workers = 2, pin_memory = True)    # CREATE THE VALIDATION LOADER.\n",
    "\n",
    "    class_weights = []                                                      # WeightedRandomSampler FOR DEALING WITH IMBALANCED CLASSES.\n",
    "    for root, subdir, files in os.walk(train_dir):                          # CREATING class_weights BY GOING THROUGH AND CHECKING EXACTLY HOW MANY FILES DO WE HAVE  OF EACH DOG BREED.\n",
    "        if len(files) > 0:                                                  # CHECK FILES ARE EXISTING IN THE DIRECTORY.\n",
    "            class_weights.append(1/len(files))                              # IF THERE ARE MORE EXAMPLES ON THAT PARTICULAR CLASS, THE WEIGHT IS GOING TO BE LOWER BECAUSE 1 IS DIVIDING BY LARGER NUMBER.\n",
    "\n",
    "    sample_weights =  [0] * len(train_dataset)\n",
    "\n",
    "    for idx, ( data, label ) in enumerate( tqdm( train_dataset.imgs )):     # WE NEED TO GO THROUGH ALL OF OUR EXAMPLES IN THE TRAINING DATASET.\n",
    "        class_weight = class_weights[label]\n",
    "        sample_weights[idx] = class_weight\n",
    "\n",
    "    sampler = WeightedRandomSampler( sample_weights, num_samples = len(sample_weights), replacement = True )         \n",
    "                                                    \n",
    "    # SPECYFING num_workers AND pin_memory FOR A LITTLE BIT OF EFFICIENCY IN DATA LOADING.\n",
    "    train_loader = DataLoader( train_dataset, batch_size = batch_size, sampler = sampler, num_workers = 2, pin_memory = True )\n",
    "                                         \n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
